{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LS_DSPT7_431_RNN_and_LSTM_Lecture.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "U4-S3-DL",
      "language": "python",
      "name": "u4-s3-dl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldr0HZ193GKb"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 4, Sprint 3, Module 1*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlEUBsRGPpv_"
      },
      "source": [
        "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
        "<br></br>\n",
        "<br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egOQuoi2Ppv_"
      },
      "source": [
        "## Learning Objectives\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IizNKWLomoA"
      },
      "source": [
        "## Overview\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44QZgrPUe3-Y"
      },
      "source": [
        "# Neural Networks for Sequences (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAAyS95APpv_"
      },
      "source": [
        "## Overview\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWrQllf8WEd-"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd76W7dfPpv_"
      },
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti23G0gRe3kr"
      },
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Dropout, SimpleRNN, LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# Set top N words and batch_size\n",
        "max_features = 20000\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "# Map for readable classnames\n",
        "class_names = [\"Negative\", \"Positive\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoFzcny8PpwA"
      },
      "source": [
        "Reviews in the IMDB dataset have been encoded as a sequence of integers. Luckily the dataset also contains an index for converting the reviews back into human readable form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoz81-WbPpwA"
      },
      "source": [
        "# Get the word index from the dataset\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Ensure that \"special\" words are mapped into human readable terms \n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNKNOWN>\"] = 2\n",
        "\n",
        "# Perform reverse word lookup and make it callable\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bfBaKWyPpwA"
      },
      "source": [
        "# Concatonate test and training datasets\n",
        "allreviews = np.concatenate((x_train, x_test), axis=0)\n",
        "\n",
        "# Review lengths across test and training whole datasets\n",
        "print(\"Maximum review length: {}\".format(len(max((allreviews), key=len))))\n",
        "print(\"Minimum review length: {}\".format(len(min((allreviews), key=len))))\n",
        "result = [len(x) for x in allreviews]\n",
        "print(\"Mean review length: {}\".format(np.mean(result)))\n",
        "\n",
        "# Print a review and it's class as stored in the dataset. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Machine readable Review\")\n",
        "print(\"  Review Text: \" + str(x_train[30]))\n",
        "print(\"  Review Sentiment: \" + str(y_train[30]))\n",
        "\n",
        "# Print a review and it's class in human readable format. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Human Readable Review\")\n",
        "print(\"  Review Text: \" + decode_review(x_train[30]))\n",
        "print(\"  Review Sentiment: \" + class_names[y_train[30]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UbbrLIPPpwA"
      },
      "source": [
        "# Get the lengths for positive and negative reviews\n",
        "all_labels = np.concatenate([y_train, y_test])\n",
        "positive = np.array(result)[all_labels==1]\n",
        "negative = np.array(result)[all_labels==0]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.distplot(positive, label='Positive Reviews', hist=False)\n",
        "sns.distplot(negative, label='Negative Reviews', hist=False)\n",
        "plt.title('Distribution of Positive and Negative Review Lengths', fontsize=14)\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABY13yNuPpwA"
      },
      "source": [
        "# If you want to pad the end of the sequences you can set padding='post'.\n",
        "maxlen = 100\n",
        "\n",
        "print('Pad Sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape: ', x_train.shape)\n",
        "print('x_test shape: ', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CSlUI9XPpwA"
      },
      "source": [
        "x_train[60]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXqu1wUNPpwA"
      },
      "source": [
        "Let's get a baseline performance for Multi-Layer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA0BqzNqPpwA"
      },
      "source": [
        "mlp = Sequential()\n",
        "\n",
        "# The Embedding Layer provides a spatial mapping (or Word Embedding) of all the \n",
        "# individual words in our training set. Words close to one another share context \n",
        "# and or meaning. This spatial mapping is learned during the training process.\n",
        "mlp.add(Embedding(max_features, 64, input_length=maxlen))\n",
        "\n",
        "# Dense layer with 128 neurons and relu activation\n",
        "mlp.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Dropout layers fight overfitting and forces the model to learn multiple \n",
        "# representations of the same data by randomly disabling neurons in the \n",
        "# learning phase.\n",
        "mlp.add(Dropout(0.25))\n",
        "\n",
        "# Final layer is dense single node. A sigmoid activation function determines \n",
        "# the output from this node - a value between 0 and 1. Closer to 0 indicates \n",
        "# a negative review. Closer to 1 indicates a positive review.\n",
        "mlp.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "mlp.compile(loss='binary_crossentropy',\n",
        "            optimizer='adam', \n",
        "            metrics=['accuracy'])\n",
        "\n",
        "mlp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AP2FDx4PpwA"
      },
      "source": [
        "mlp_history = mlp.fit(x_train, y_train,\n",
        "                      batch_size=batch_size, \n",
        "                      epochs=5, \n",
        "                      validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaUx9_TnPpwA"
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(mlp_history.history['accuracy'])\n",
        "plt.plot(mlp_history.history['val_accuracy'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDZGLkMhPpwB"
      },
      "source": [
        "Let's try using the SimpleRNN layers instead of Dense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WifhmSSnPpwB"
      },
      "source": [
        "rnn = Sequential()\n",
        "rnn.add(Embedding(max_features, 64, input_length=maxlen))\n",
        "rnn.add(SimpleRNN(100))\n",
        "rnn.add(Dropout(0.25))\n",
        "rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "rnn.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "rnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZbsEQnOPpwB"
      },
      "source": [
        "rnn_history = mlp.fit(x_train, y_train,\n",
        "                      batch_size=batch_size, \n",
        "                      epochs=5, \n",
        "                      validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7l5mOtDPpwB"
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(rnn_history.history['accuracy'])\n",
        "plt.plot(rnn_history.history['val_accuracy'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSN1kVDXPpwB"
      },
      "source": [
        "Lastly, let's try replacing the SimpleRNN layer with LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYWxzkN2PpwB"
      },
      "source": [
        "lstm = Sequential()\n",
        "lstm.add(Embedding(max_features, 128, input_length=maxlen))\n",
        "lstm.add(LSTM(128))\n",
        "lstm.add(Dropout(0.25))\n",
        "lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "lstm.compile(loss='binary_crossentropy',\n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])\n",
        "lstm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrE6XsqqPpwB"
      },
      "source": [
        "lstm_history = lstm.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, \n",
        "                        epochs=5, \n",
        "                        validation_data=(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueo6DJ2SPpwB"
      },
      "source": [
        "# Plot training & validation accuracy values\n",
        "plt.plot(lstm_history.history['accuracy'])\n",
        "plt.plot(lstm_history.history['val_accuracy'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fgTDw9-PpwB"
      },
      "source": [
        "## Evaluate model with test data and view results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCRor4d9PpwB"
      },
      "source": [
        "# Get Model Predictions for test data\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicted_classes = lstm.predict_classes(x_test)\n",
        "print(classification_report(y_test, predicted_classes, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDaXDQIKPpwB"
      },
      "source": [
        "## View some incorrect predictions\n",
        "\n",
        "Lets have a look at some of the incorrectly classified reviews. For readability we remove the padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4TcsBmYPpwB"
      },
      "source": [
        "predicted_classes_reshaped = np.reshape(predicted_classes, 25000)\n",
        "\n",
        "incorrect = np.nonzero(predicted_classes_reshaped != y_test)[0]\n",
        "\n",
        "# We select the first 10 incorrectly classified reviews\n",
        "for j, incorrect in enumerate(incorrect[0:3]):\n",
        "    \n",
        "    predicted = class_names[predicted_classes_reshaped[incorrect]]\n",
        "    actual = class_names[y_test[incorrect]]\n",
        "    human_readable_review = decode_review(x_test[incorrect])\n",
        "    \n",
        "    print(\"Incorrectly classified Test Review [\"+ str(j+1) +\"]\") \n",
        "    print(\"Test Review #\" + str(incorrect)  + \": Predicted [\"+ predicted + \"] Actual [\"+ actual + \"]\")\n",
        "    print(\"Test Review Text: \" + human_readable_review.replace(\"<PAD> \", \"\"))\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAJI9UJqPpwB"
      },
      "source": [
        "## Run your own text against the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY_l7N-lPpwB"
      },
      "source": [
        "# Write your own review\n",
        "review = \"this is the best film i have ever seen it is great and fantastic and i loved it\"\n",
        "\n",
        "# Encode review (replace word with integers)\n",
        "tmp = []\n",
        "for word in review.split(\" \"):\n",
        "    tmp.append(word_index[word])\n",
        "\n",
        "# Ensure review is 300 words long (by padding or truncating)\n",
        "tmp_padded = sequence.pad_sequences([tmp], maxlen=maxlen) \n",
        "\n",
        "# Run your processed review against the trained model\n",
        "rawprediction = lstm.predict(np.array([tmp_padded][0]))[0][0]\n",
        "prediction = int(round(rawprediction))\n",
        "\n",
        "# Test the model and print the result\n",
        "print(\"Review: \" + review)\n",
        "print(\"Raw Prediction: \" + str(rawprediction))\n",
        "print(\"Predicted Class: \" + class_names[prediction])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ohxwsqPpwB"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. \n",
        "\n",
        "Reference for some of the code in the section above can be found: https://github.com/markwest1972/LSTM-Example-Google-Colaboratory/blob/master/LSTM_IMDB_Sentiment_Example.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pETWPIe362y"
      },
      "source": [
        "# LSTM Text generation with Keras (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c27psrt1PpwB"
      },
      "source": [
        "## Overview\n",
        "\n",
        "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I've pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
        "\n",
        "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dwqEl_VPpwB"
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaungR-5PpwB"
      },
      "source": [
        "data_files = os.listdir('./articles')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwQ7McMdPpwB"
      },
      "source": [
        "# Read in Data\n",
        "\n",
        "data = []\n",
        "\n",
        "for file in data_files:\n",
        "    if file[-3:] == 'txt':\n",
        "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
        "            data.append(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9g7GPvqPpwB"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgQPT-fkPpwB"
      },
      "source": [
        "data[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPIavuWvPpwB"
      },
      "source": [
        "# Encode Data as Chars\n",
        "\n",
        "# Gather all text \n",
        "# Why? 1. See all possible characters 2. For training / splitting later\n",
        "text = \" \".join(data)\n",
        "\n",
        "# Unique Characters\n",
        "chars = list(set(text))\n",
        "\n",
        "# Lookup Tables\n",
        "char_int = {c:i for i, c in enumerate(chars)} \n",
        "int_char = {i:c for i, c in enumerate(chars)} "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqfXeEkZPpwB"
      },
      "source": [
        "len(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7d1lvGePpwB"
      },
      "source": [
        "# Create the sequence data\n",
        "maxlen = 40\n",
        "step = 5\n",
        "\n",
        "encoded = [char_int[c] for c in text]\n",
        "\n",
        "sequences = [] # Each element is 40 chars long\n",
        "next_char = [] # One element for each sequence\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "    \n",
        "    sequences.append(encoded[i : i + maxlen])\n",
        "    next_char.append(encoded[i + maxlen])\n",
        "    \n",
        "print('sequences: ', len(sequences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQN8DqiFPpwB"
      },
      "source": [
        "sequences[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEtegugtPpwB"
      },
      "source": [
        "# Create x & y\n",
        "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i,t,char] = 1\n",
        "    y[i, next_char[i]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJLo0oaSPpwB"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8M-w1D1PpwB"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IU-LTJVPpwB"
      },
      "source": [
        "# build the model: a single LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNS_7G1FPpwB"
      },
      "source": [
        "def sample(preds):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / 1\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IlNqGf3PpwB"
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    \n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "    \n",
        "    start_index = random.randint(0, len(text) - maxlen - 1) \n",
        "    generated = ''\n",
        "    \n",
        "    sentence = text[start_index: start_index + maxlen]\n",
        "    generated += sentence\n",
        "    \n",
        "    print('----- Generating with seed: \"' + sentence + '\"')\n",
        "    sys.stdout.write(generated)\n",
        "    \n",
        "    for i in range(400):\n",
        "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x_pred[0, t, char_int[char]] = 1\n",
        "            \n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_char[next_index]\n",
        "        \n",
        "        sentence = sentence[1:] + next_char\n",
        "        \n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Xi4qzePpwB"
      },
      "source": [
        "# fit the model\n",
        "model.fit(x, y,\n",
        "          batch_size=32,\n",
        "          epochs=10,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUqdR_hvPpwB"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to use a Keras LSTM to generate text on today's assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ydmSSGPpwB"
      },
      "source": [
        "# Review\n",
        "\n",
        "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
        "    * Sequence Problems:\n",
        "        - Time Series (like Stock Prices, Weather, etc.)\n",
        "        - Text Classification\n",
        "        - Text Generation\n",
        "        - And many more! :D\n",
        "    * LSTMs are generally preferred over RNNs for most problems\n",
        "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
        "    * Keras has LSTMs/RNN layer types implemented nicely\n",
        "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
        "    * Shape of input data is very important\n",
        "    * Can take a while to train\n",
        "    * You can use it to write movie scripts. :P "
      ]
    }
  ]
}